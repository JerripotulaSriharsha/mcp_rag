{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92ac21af",
   "metadata": {},
   "source": [
    "Bi Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07c0358e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/26/25 15:01:46] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.openai.com/v1/embeddings</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 </span> <a href=\"file://d:\\Narwal\\mcp_rag\\.venv\\Lib\\site-packages\\httpx\\_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Narwal\\mcp_rag\\.venv\\Lib\\site-packages\\httpx\\_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">OK\"</span>                                                                    <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/26/25 15:01:46]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttps://api.openai.com/v1/embeddings\u001b[0m \u001b[32m\"HTTP/1.1 200 \u001b[0m \u001b]8;id=526031;file://d:\\Narwal\\mcp_rag\\.venv\\Lib\\site-packages\\httpx\\_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=240087;file://d:\\Narwal\\mcp_rag\\.venv\\Lib\\site-packages\\httpx\\_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32mOK\"\u001b[0m                                                                    \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">GET</span>                                                      <a href=\"file://d:\\Narwal\\mcp_rag\\.venv\\Lib\\site-packages\\httpx\\_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Narwal\\mcp_rag\\.venv\\Lib\\site-packages\\httpx\\_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://07179cff-5524-4d69-b685-ff8e8896e747.us-east4-0.gcp.cloud.qdra</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">nt.io:6333</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>                                           <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mGET\u001b[0m                                                      \u001b]8;id=877612;file://d:\\Narwal\\mcp_rag\\.venv\\Lib\\site-packages\\httpx\\_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=380001;file://d:\\Narwal\\mcp_rag\\.venv\\Lib\\site-packages\\httpx\\_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttps://07179cff-5524-4d69-b685-ff8e8896e747.us-east4-0.gcp.cloud.qdra\u001b[0m \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;94mnt.io:6333\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m                                           \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/26/25 15:01:47] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span>                                                     <a href=\"file://d:\\Narwal\\mcp_rag\\.venv\\Lib\\site-packages\\httpx\\_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Narwal\\mcp_rag\\.venv\\Lib\\site-packages\\httpx\\_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://07179cff-5524-4d69-b685-ff8e8896e747.us-east4-0.gcp.cloud.qdra</span> <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">nt.io:6333/collections/rag_mcp/points/query</span> <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>          <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/26/25 15:01:47]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m                                                     \u001b]8;id=905974;file://d:\\Narwal\\mcp_rag\\.venv\\Lib\\site-packages\\httpx\\_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=657572;file://d:\\Narwal\\mcp_rag\\.venv\\Lib\\site-packages\\httpx\\_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;94mhttps://07179cff-5524-4d69-b685-ff8e8896e747.us-east4-0.gcp.cloud.qdra\u001b[0m \u001b[2m               \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[4;94mnt.io:6333/collections/rag_mcp/points/query\u001b[0m \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m          \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from query import query_qdrant\n",
    "bi_encoder_retrival = query_qdrant(query_text = \"By what factor does the proposed 3D representation in PhysX-Anything reduce the number of geometry tokens?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5048f046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ScoredPoint(id='9f073b3b-9107-4aa9-923e-2604298c0395', version=11, score=0.8305408, payload={'text': '-\\nsidering both geometry and all physical attributes.                                In to-\\ntal, we collect 1,568 valid scores from 14 volunteers and\\nnormalize the scores. The results show that the outputs of\\nPhysX-Anything align much better with human preferences\\nthan those of other methods, confirming its robust gener-\\native performance in both geometry and physical proper-\\n\\n                                                                                                     6\\n', 'metadata': {'file_path': 'D:\\\\Narwal\\\\mcp_rag\\\\data\\\\paper1.pdf', 'file_name': 'paper1.pdf', 'file_type': 'application/pdf', 'file_size': 4994615, 'creation_date': '2025-12-22', 'last_modified_date': '2025-12-22'}, 'chunk_index': 1, 'source_doc': 'dca54582-14cc-44a2-acaa-a22cbd7cdeb9'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='be308b15-3fff-4a3e-ac41-a95f93944955', version=14, score=0.8228432, payload={'text': '\\n    Input    PhysX-Anything (Voxel)    PhysX-Anything (Index)    PhysX-Anything (Ours)\\n             Geometry    Kinematic     Geometry    Kinematic     Geometry    Kinematic\\n\\n              Question: Find Dimension: 90*60*100                              Dimension: 90*60*100                                        Dimension: 90*60*100\\n              a wheel of the shopping cart made of rubber                      Question: Find a wheel of the shopping cart made of rubber  Question: Find a wheel of the shopping cart made of rubber\\n                             1                      1                   7.8    1                 1                         7.8             1                    1               7.8\\n                             0.5                    0.5                 3.4    0.5               0.5                       3.4             0.5                  0.5             3.4\\n\\n              Affordance     0      Description     0      Material     1.1    Affordance  0     0                         1.1             0                    0               1.1\\n                                                                                                Description    Material                    Affordance    Description    Material\\n\\n    Figure 7. Ablation studies on different representation. We compare the generative performance of different 3D representations, which\\n    validates both the effectiveness and efficiency of our proposed representation.\\n              Faucet Switch Manipulation                                       Door Opening and Closing                                    Eyeglass Temple Folding\\n    #2        #32                   #63                    #23                 #53                    #67                  #13                           #29            #71\\n\\n    Lighter Snapping Open    Laptop Closing    Handle Manipulation\\n    #8    #31    #77         #8    #43         #70    #15    #42    #86\\n\\n    Figure 8.  Robot Manipulation on Generated sim-reaady 3D assets of PhysX-Anything. The results show that our generated sim-ready\\n    assets exhibit highly physically plausible behavior and accurate geometric structure across diverse tasks, thereby providing a new direction\\n    for robotics policy learning.\\n    Table 5.   Comparison with different representations. Quantitative results across different 3D representations clearly demonstrate the\\n    superiority of our proposed representation in both geometric fidelity and physical attributes.\\n\\n    Methods                  PSNR ↑ Geometry                          Physical Attributes\\n                                       CD ↓    F-Score ↑    Absolute scale ↓    Material ↑    Affordance ↑    Kinematic parameters (VLM) ↑    Description ↑\\n    PhysX-Anything-Voxel     16.96     17.81     63.10            0.40            12.32          11.63                    0.39                    17.38\\n    PhysX-Anything-Index     18.21     16.27     68.70            0.30            13.35          12.04                    0.76                    17.97\\n    PhysX-Anything (Ours)    20.35     14.43     77.50            0.30            17.52          14.28                    0.94                    19.36\\n\\nnets, lighters, eyeglasses, and other everyday objects—can\\nbe directly imported into the simulator and used for contact-\\nrich robotics policy learning.     This experiment not only\\ndemonstrates the physically plausible behavior and accu-\\nrate geometry of our generated assets, but also highlights\\ntheir strong potential to enable and inspire a wide range of\\ndownstream robotics and embodied AI applications.\\n\\n5. Conclusion\\n\\nIn this paper, we aim to fully unlock the potential of syn-\\nthesized 3D assets in real-world applications by introduc-\\ning PhysX-Anything, the first sim-ready physical 3D gen-\\nerative paradigm. Through a unified VLM-based pipeline\\nand a tailored 3D representation, PhysX-Anything achieves\\nsubstantial token compression (over 193×) while preserv-\\ning explicit geometric structure, enabling efficient and scal-\\nable physical 3D generation. In addition, to enrich the diver-\\nsity of existing physical 3D datasets, we construct PhysX-\\nMobility by carefully collecting and annotating common\\nreal-world objects with rich physical attributes.             It in-\\ncludes 47 the most common real-life categories with de-\\ntailed physical attributes.             Comprehensive experiments on\\nPhysX-Mobility and in-the-wild scenarios demonstrate the\\nstrong performance and robust generalization of PhysX-\\nAnything in sim-ready physical 3D generation.        Further-\\nmore, simulation-based experiments highlight its potential\\nfor downstream robotic policy learning. We believe PhysX-\\nAnything will spur new research directions across 3D vi-\\nsion, embodied AI and robotics.\\n\\n                                                                     ', 'metadata': {'file_path': 'D:\\\\Narwal\\\\mcp_rag\\\\data\\\\paper1.pdf', 'file_name': 'paper1.pdf', 'file_type': 'application/pdf', 'file_size': 4994615, 'creation_date': '2025-12-22', 'last_modified_date': '2025-12-22'}, 'chunk_index': 0, 'source_doc': 'a91d119e-6b89-4567-91d0-207c0186f03b'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='d49cdab8-96e7-4545-a22b-8aec085e441f', version=12, score=0.82011235, payload={'text': '\\n    Input Image    URDFormer    Articulate-Anything    PhysXGen    PhysX-Anything (Ours)\\n                   Geometry    Kinematic    Geometry    Kinematic    Geometry    Kinematic    Geometry    Kinematic\\n\\n                                                         PhysXGen                                                                                         PhysX-Anything (Ours)\\n                      Dimension: 79.6*56.85*4.54         Question: Find the main ceramic structure of the toilet.            Dimension: 35*25*2           Question: Find the main ceramic structure of the toilet.\\n     1                                                                               1                               2.2                           1                           1                                  2.4\\n\\n                                                  0.5                   0.5                                          1.2                           0.5                         0.5                                1.2\\n\\n     0                                                                               0                               0.2                           0                           0                                  0\\n                       Affordance                        Description           Material                                      Affordance                   Description                 Material\\n                      Dimension: 58.6*47.3*9.97          Question: Find the metal outlet at the end of the tube              Dimension: 30*25*5           Question: Find the metal outlet at the end of the tube\\n     1                                                                               1                               7.6                           1                           1                                  7.8\\n\\n                                                  0.5                   0.5                                          5.25                          0.5                         0.5                                5.3\\n\\n     0                                                                               0                               2.9                           0                           0                                  2.9\\n                       Affordance                        Description           Material                                      Affordance                   Description                 Material\\n    Figure 6.   Qualitative results on in-the-wild images. Given a single real-world image as input, PhysX-Anything produces high-quality\\n    sim-ready 3D assets with realistic geometry, articulation, and physical attributes across diverse object categories. Moreover, the results\\n    highlight the robust generalization of PhysX-Anything.\\n\\n    Table 3.    User studies on in-the-wild evaluation.                 User preference results on in-the-wild cases show that PhysX-Anything significantly\\n    outperforms other methods, achieving a clear margin of improvement in geometry quality and physical plausibility.\\n\\n     Methods              Geometry (Human) ↑                            Absolute scale ↑    Material ↑                       Physical Attributes (Human)\\n                                                                                                                     Affordance ↑                  Kinematic parameters ↑      Description ↑\\n     URDFormer [11]              0.21                                          –                –                         –                                 0.23                     –\\n     Articulate-Anything [16]    0.53                                          –                –                         –                                 0.37                     –\\n     PhysXGen [3]                0.61                                         0.48             0.43                      0.34                               0.32                    0.33\\n     PhysX-Anything (Ours)       0.98                                         0.95             0.84                      0.94                               0.98                    0.96\\n\\n    Table 4.    In-the-wild VLM-based evaluation.                       Quantitative re-                           Fig. 3. Note that the original mesh and vertex-quantization\\n    sults from GPT-5 also confirm the strong generative performance                                                representations require an excessively large number of to-\\n    of PhysX-Anything in terms of geometry and articulation.                                                       kens, making end-to-end training infeasible due to out-of-\\n\\nMethods               Geometry (VLM) ↑                              Kinematic parameters (VLM) ↑\\n\\nURDFormer [11]              0.29                                                0.31\\nArticulate-Anything [16]    0.61                                                0.64\\nPhysXGen [3]                0.65                                                0.61\\nPhysX-Anything (Ours)       0.94                                                0.94\\n\\nties.  The visualizations in Figure 6 on real-life scenarios\\nfurther highlight the superiority of PhysX-Anythingagainst\\nother methods, showing more accurate geometry, articula-\\ntion, and physical attributes across diverse and challenging\\nin-the-wild cases.\\nmemory issues.          Therefore, we focus our comparison on\\nthe remaining three compact representations. As shown in\\nTable 5, as the token compression ratio increases, PhysX-\\nAnything is able to capture complete and detailed geometry\\n\\neven for complex structures, whereas alternative represen-\\ntations are constrained by the token budget and suffer no-\\nticeable degradation. The qualitative results in Fig. 7 fur-\\nther show that our PhysX-Anything produces more robust\\nresults for geometrically challenging objects.\\n\\n4.4. Robotic Policy Learning in Simulation\\n\\n    4.3.', 'metadata': {'file_path': 'D:\\\\Narwal\\\\mcp_rag\\\\data\\\\paper1.pdf', 'file_name': 'paper1.pdf', 'file_type': 'application/pdf', 'file_size': 4994615, 'creation_date': '2025-12-22', 'last_modified_date': '2025-12-22'}, 'chunk_index': 0, 'source_doc': '981b41d2-1792-477e-862a-e627634cfc9f'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='067e42e0-bc43-41e2-b24c-a5a4c56af6e9', version=3, score=0.8141379, payload={'text': ' for\\n• downstream applications in simulation and embodied AI.\\n We propose a unified VLM-based generative pipeline to-\\n gether with a    novel physical 3D representation. Our\\n representation compresses geometry tokens at a high rate\\n while preserving explicit geometric structure, and avoids\\n• introducing any special tokens during fine-tuning.\\n We construct a new physically grounded 3D dataset,\\n PhysX-Mobility, which enriches the category diversity\\n of existing physical 3D datasets by over  2×, including\\n over 2K common real-world objects such as cameras, cof-\\n• fee machines, and staplers.\\n Through comprehensive evaluations on PhysX-Mobility\\n and in-the-wild images, we demonstrate the strong gen-\\n erative quality  and robust generalization of      PhysX-\\n Anything. Furthermore, we validate the feasibility of di-\\n rectly deploying our sim-ready assets in simulation envi-\\n ronments, thereby empowering downstream applications\\n such as embodied AI and robotic manipulation.\\n2. Related Works\\n\\n2.1. 3D Generative Models\\n\\nAs one of the earliest paradigms for 3D generation, gen-\\nerative adversarial networks (GANs) played a central role\\nin the early stage of this field [6, 13].  However, they\\n\\n                                                                         2\\n', 'metadata': {'file_path': 'D:\\\\Narwal\\\\mcp_rag\\\\data\\\\paper1.pdf', 'file_name': 'paper1.pdf', 'file_type': 'application/pdf', 'file_size': 4994615, 'creation_date': '2025-12-22', 'last_modified_date': '2025-12-22'}, 'chunk_index': 1, 'source_doc': '08a36f40-bc13-4cd4-a5a0-4be26091fe6f'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='87d0c3a1-e8a3-4654-9a7f-6b9624e4b07d', version=7, score=0.8069593, payload={'text': ' robustly generalize to novel\\nstructures, unseen categories, and complex texture. Drea-\\nmArt [21] instead attempts to optimize articulated 3D ob-\\njects from video generation outputs, but it requires manually\\nannotated part masks and becomes unstable when handling\\nobjects with many movable parts. URDF-Anything [19] can\\ndirectly generate URDF files. However, it relies on robust\\npoint cloud inputs and is hard to generate detailed texture\\nfor 3D assets.            Although some works attempt to learn the\\nphysical deformation of 3D assets [7, 8, 15, 17], they either\\ntreat all objects as homogeneous or ignore some key physi-\\ncal attributes. To push 3D generation toward physical real-\\nism, PhysXGen [3] first proposes a unified framework that\\ndirectly generates 3D assets with essential physical proper-\\nties, including absolute dimension, density, and so on. De-\\nspite its promising performance in physical 3D generation,\\nthere remains a substantial gap between the synthesized as-\\nsets and the requirements of modern physics simulators, re-\\nsulting in limited direct usability in downstream tasks.\\nTo fully realize the downstream utility of synthetic 3D\\nassets, we introduce the first 3D generation paradigm that,\\nfrom a single real-world image, produces high-quality sim-\\nready 3D assets equipped with explicit physical properties.\\nWe compare PhysX-Anything with existing approaches in\\nTable 1, which highlights that our method is the only one\\nthat simultaneously supports articulation, physical model-\\n\\ning, strong generalization, and simulation-ready deploy-\\n\\n\\nment. We believe that our approach offers a new direction\\n\\n\\nfor using synthetic data to empower related applications.\\n\\n3. Methodology\\n\\nIn this section, we present the detailed paradigm of PhysX-\\n\\nAnything, as illustrated in Fig. 3. It adopts a global-to-local\\n\\npipeline. Specifically, given a real-world image, PhysX-\\n\\nAnything conducts a multi-round conversation to sequen-\\ntially generate the overall physical description and the geo-\\nmetric information of each part. To mitigate context forget-\\nting caused by overly long prompts, we retain only the over-\\nall information when generating per-part geometry. In other\\nwords, the geometric descriptions of different parts are gen-\\nerated independently, conditioned solely on the shared over-\\nall information. Finally, by decoding the physical represen-\\ntation, PhysX-Anything can output simulation-ready physi-\\ncal 3D assets in six commonly used formats.\\n3.1. Physical Representation\\nPreviously, to reduce the token length of raw 3D meshes\\nin VLM-based frameworks, most 3D generation meth-\\nods [12, 26] adopt text-serialized representations based on\\nvertex quantization. However, the resulting token sequences\\nremain excessively long. Although 3D VQ-GAN [31] can\\nfurther compress geometric tokens, it requires introducing\\nadditional special tokens and a customized tokenizer during\\nfine-tuning, which complicates training and deployment.\\nTo address these limitations, we propose a new 3D rep-\\nresentation that substantially reduces token length while\\npreserving explicit geometric structure, without introducing\\nany additional tokenizer. Motivated by the impressive trade-\\noff between fidelity and efficiency of voxel-based represen-\\ntations [28], we build our representation on voxels. Directly\\nencoding high-resolution voxels, however, still yields an\\nunaffordable number of tokens for VLMs, even after map-\\nping geometry to a compressed space. We therefore adopt\\na coarse-to-fine strategy for geometry modeling: the VLM\\noperates on a 323 voxel grid to capture coarse geometry,\\nwhile a downstream decoder refines this coarse shape into\\nhigh-fidelity geometry. In this way, we retain the explicit\\nstructural advantages of 3D voxels while avoiding exces-\\nsive token consumption. As shown in Fig. 3, converting\\nmeshes to coarse voxels alone reduces the number of to-\\nkens by 74×.  To further eliminate redundancy in sparse\\nvoxel data, we linearize the 323 grid into indices from 0\\nto 323 − 1 and serialize only occupied voxels. Finally, by\\nmerging neighboring occupied indices and connecting con-\\ntinuous ranges with a hyphen −, we achieve an even higher\\ntoken compression rate (193×) while maintaining explicit\\n\\n                                                                                                                             4\\n', 'metadata': {'file_path': 'D:\\\\Narwal\\\\mcp_rag\\\\data\\\\paper1.pdf', 'file_name': 'paper1.pdf', 'file_type': 'application/pdf', 'file_size': 4994615, 'creation_date': '2025-12-22', 'last_modified_date': '2025-12-22'}, 'chunk_index': 1, 'source_doc': '59308638-c26d-4533-9909-fff3e971ef2a'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='afc0882b-4ca4-4405-8ca9-4bcd500c2409', version=2, score=0.80556536, payload={'text': '\\n    range of downstream applications, especially in embodied              corresponding URDF & XML structure, yielding sim-ready\\n    AI and physics-based simulation.                                      assets that can be directly imported into standard simulators.\\n                                                                           Additionally, to significantly enrich the diversity of ex-\\n\\n1. Introduction\\n\\nFor a broad range of downstream applications in robotics,\\nembodied AI, and interactive simulation, there is an increas-\\ning demand for high-quality physical 3D assets that can be\\ndirectly executed in simulators. However, most existing 3D\\ngeneration methods either focus on global 3D geometry and\\nvisual appearance [10, 12, 14, 26, 28, 31], or on part-aware\\ngeneration [30, 33] that models object hierarchies and fine-\\ngrained structures. Despite their visually impressive perfor-\\nmance, the resulting assets typically lack essential physi-\\ncal and articulation information—such as density, absolute\\nscale, and joint constraints—which creates a substantial gap\\nto real-world applications and makes these assets difficult to\\ndeploy directly in simulators or physics engines.\\nIn parallel, a few works have started to explore the gener-\\nation of articulated objects [11, 16, 20, 21]. Yet, due to the\\nscarcity of large-scale high-quality annotated 3D datasets,\\nmany of these methods adopt retrieval-based paradigms:\\nthey retrieve an existing 3D model and attach plausible\\nmotions, rather than synthesizing fully novel, physically\\ngrounded assets. As a result, they provide only limited ar-\\nticulation information, generalize poorly to in-the-wild im-\\nages, and still lack the physical attributes required for realis-\\ntic simulation. While prior efforts attempt to learn deforma-\\ntion behavior for 3D assets [7, 8, 15, 17], they often impose\\na homogeneous-material assumption or neglect some es-\\nsential physical attributes. Even PhysXGen [3], which can\\ndirectly generate physical 3D assets, does not yet support\\nplug-and-play deployment in standard simulators or physics\\nengines [25, 27], thereby constraining its practical utility for\\ndownstream embodied AI and control tasks.\\nTo bridge  the     gap between      synthetic  3D    assets\\nand real downstream applications,   we propose       PhysX-\\nAnything—the first simulation-ready (sim-ready) phys-\\nical 3D generative paradigm. Given a single in-the-wild\\nimage, PhysX-Anything produces a high-quality sim-ready\\n3D asset, as illustrated in Fig. 1. Specifically, we introduce\\nthe first unified VLM-based generative model that jointly\\npredicts geometry, articulation structure, and essential phys-\\nical properties. Meanwhile, to resolve the intrinsic tension\\nbetween the limited token budget of VLMs and the com-\\nplexity of detailed 3D geometry, we design a new 3D rep-\\nresentation that tokenizes geometry efficiently. This repre-\\nsentation reduces the number of tokens by 193×, making it\\nfeasible to learn explicit geometry directly while avoiding\\nthe introduction of special tokens and new tokenizer during\\nfine-tuning. Based on the coarse geometry generated by the\\nVLM, we further develop a controllable flow transformer\\nand decoder to synthesize fine-grained geometry and the\\nisting physically grounded 3D datasets [3], we build a\\nnew dataset, PhysX-Mobility, by collecting assets from\\nPartNet-Mobility [27] and carefully annotating their phys-\\nical attributes.  PhysX-Mobility spans 47 categories and\\ncovers common real-world objects such as toilets, fans,\\ncameras, coffee machines, and staplers, thereby substan-\\ntially broadening the category coverage of physical 3D as-\\nsets. Comprehensive experiments on PhysX-Mobility, in-\\nthe-wild images, and user studies demonstrate that PhysX-\\nAnything achieves strong generative quality and robust gen-\\neralization compared with recent state-of-the-art methods.\\nFurthermore, to validate executability in standard simu-\\nlators and physics engines, we conduct experiments in a\\nMuJoCo-style simulator, showing that our sim-ready assets\\ncan be directly used in robotic policy learning for contact-\\nrich tasks, such as safe manipulation of delicate objects like\\neyeglasses. We believe our work opens up new possibilities\\nand directions for future research in 3D generation, embod-\\nied AI, and robotics.\\n• To summarize, our main contributions are:\\n We introduce PhysX-Anything, the first sim-ready phys-\\n ical 3D generative paradigm that,  given a single in-\\n the-wild image, produces high-quality sim-ready 3D as-\\n sets, thereby pushing the frontier of physically grounded\\n 3D content creation and unlocking new possibilities', 'metadata': {'file_path': 'D:\\\\Narwal\\\\mcp_rag\\\\data\\\\paper1.pdf', 'file_name': 'paper1.pdf', 'file_type': 'application/pdf', 'file_size': 4994615, 'creation_date': '2025-12-22', 'last_modified_date': '2025-12-22'}, 'chunk_index': 0, 'source_doc': '08a36f40-bc13-4cd4-a5a0-4be26091fe6f'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='308be642-05ba-479c-9be9-a4d0a97efde6', version=5, score=0.80490094, payload={'text': ' introduced the SDS loss, which lever-\\nages the strong prior of 2D diffusion models to achieve\\nimpressive text-driven 3D generation quality.\\nless, optimization-based methods still suffer from the multi-\\nface Janus problem and low optimization efficiency.\\ncently, feed-forward methods have become the mainstream\\nin 3D generation due to their favorable efficiency and ro-\\nbustness [2, 4, 5, 10, 14, 24, 28, 29].\\ngenerative              per-          based models, several works introduce autoregressive mod-\\n             Subsequently,            eling into 3D generation [9, 23]. Motivated by the strong\\n                                      performance of vision–language models (VLMs), recent\\n                                      approaches have begun to employ VLMs to generate 3D as-\\n                        Neverthe-     sets. To limit the token length, LLaMA-Mesh [26] adopts a\\n                                      simplified mesh representation, upon which MeshLLM [12]\\n                              Re-     builds a part-to-assembly pipeline to further improve gen-\\n                                      erative quality.     Instead of using a simplified mesh repre-\\n                                      sentation, ShapeLLM-Omni [31] adopts a 3D VQ-VAE to\\nBeyond diffusion-                     compress the token sequence length, but at the cost of in-\\n\\n    Table 1.                                            Comparison of representative methods and their capa-     troducing additional special tokens and a new tokenizer for\\n    bilities. Gen. represents the generalization of methods. It shows                                            geometry, which complicates the training procedure.\\n    that our PhysX-Anything is the only approach that simultaneously                                                   In contrast to prior work, to better unlock the potential of\\n    satisfies all four criteria.                                                                                 VLMs for 3D generation, we propose a new, efficient repre-\\n\\nMethods                              Paradigm Articulate Physical Gen. Sim-ready\\nURDFormer .etc [11, 16, 20]  Retrieval    ✓    ✗    ✗    ✗\\nTrellis .etc [10, 14, 28]    Diffusion    ✗    ✗    ✓    ✗\\nMeshLLM .etc [12, 26, 31]       VLM       ✗    ✗    ✓    ✗\\nPhysXGen [3]                 Diffusion    ✓    ✓    ✓    ✗\\nPhysX-Anything                  VLM       ✓    ✓    ✓    ✓\\nsentation that substantially compresses the token sequence\\nwhile preserving explicit structural information. Moreover,\\nour approach introduces no additional special tokens during\\nfine-tuning, thereby avoiding both the need for large-scale\\n\\ntask-specific pretraining datasets and the overhead of train-\\n\\n    3\\n', 'metadata': {'file_path': 'D:\\\\Narwal\\\\mcp_rag\\\\data\\\\paper1.pdf', 'file_name': 'paper1.pdf', 'file_type': 'application/pdf', 'file_size': 4994615, 'creation_date': '2025-12-22', 'last_modified_date': '2025-12-22'}, 'chunk_index': 1, 'source_doc': '1d9d50e5-db24-48db-8473-dc37d483c721'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='ada3ac08-5b96-45f0-8e2c-bd1bf3508ed7', version=9, score=0.80384004, payload={'text': 'ation, fine-grained voxel target, Gaussian noise, im-                 Beyond the quantitative comparison, we further present\\n    age condition, time step, and the controllable flow trans-              qualitative results in Fig. 5.     It clearly highlight the supe-\\n    former parameterized by θ, respectively. The noisy sample               riority of PhysX-Anything in terms of generalization, es-\\n    xt is obtained by interpolating between x0 and ϵ, i.e.,xt         =     pecially when compared with retrieval-based methods [11,\\n    (1 − t)x0 + tϵ .                                                        16].      Leveraging the powerful VLM prior and efficient\\n    Given the fine-grained voxel representation, we adopt a                 representation, PhysX-Anything also produces significantly\\n    pre-trained structured latent diffusion model [28] to gen-              more plausible physical attributes than PhysXGen [3].\\n\\n                                                                           5\\n', 'metadata': {'file_path': 'D:\\\\Narwal\\\\mcp_rag\\\\data\\\\paper1.pdf', 'file_name': 'paper1.pdf', 'file_type': 'application/pdf', 'file_size': 4994615, 'creation_date': '2025-12-22', 'last_modified_date': '2025-12-22'}, 'chunk_index': 1, 'source_doc': 'eb4ecc3c-0a38-4f44-be93-648d014e9573'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='68da06a5-f6cf-4b48-80e4-d3469c21258d', version=6, score=0.80216414, payload={'text': '\\n                 Avg Token of Parts=177450                           Avg Token of Part=118490\\nv    ⎵      0 .       0 2  ⎵ 0 .  7 1     ⎵     0  .    0  5         v     ⎵    8  ⎵  3     0       ⎵    1    2\\n     ⎵                     ⎵              ⎵                     Quantization\\nv           0 .       0 3    0 .  7 2           0  .    0  2         v     ⎵    1  3  ⎵     3       1    ⎵    1     2\\nMesh ⎵                     ⎵              ⎵                          v     ⎵ ⎵        ⎵ ⎵                ⎵ ⎵\\nv           0 .       0 3    0 .  7 2           0  .    0  3                    1  3        3       1         1     2\\n     ⎵                     ⎵              ⎵                                ⎵          ⎵                  ⎵\\nOriginal                                                             v\\nv           0 .       0 3    0 .  7 2           0  .    0  4                    1  3        3       1         1     2\\n     ⎵                     ⎵              ⎵                          v     ⎵          ⎵                  ⎵\\nv           0 .       0 8    0 .  7 4           0  .    0  2 Vertex             1  3        3       1         1     2\\n…                                                                    …\\nf    ⎵      1 ⎵       2 ⎵  3        ⎵ represents Spaces              f     ⎵    1  ⎵  2          ⎵  3\\n…                                                                    …\\n            Avg Token of Part=2377  Avg Token of Parts=1628                            Avg Token of Part=919\\n8    ⎵      3 0       ⎵ 1  2 ⎵           9     1  6    4   ⎵                     9 1   6    4       ⎵\\n            ⎵           ⎵      ⎵  Voxel index                   ⎵\\nVoxelization  3       1    1 2           1     4  3    1   6                     1 4   3    1       6    -\\n1    3      ⎵    ⎵        ⎵                       ⎵             ⎵         Ours        ⎵                  ⎵\\n1    3      ⎵ 3       1 ⎵  1 3 ⎵         1     4  3    1   7    ⎵                1 4   3    1       8\\n1    3      ⎵ 3       1 ⎵  1 4 ⎵         1     4  3    1   8    ⎵                2…3   3    5       5    ⎵\\n2    2      ⎵ 2       5 ⎵  2 7 ⎵         2     3  3    5   5    ⎵\\n…                                        …                                ~193 × Tokens Reduction\\nFigure 3.        Comparison of token counts between representa-\\ntions.                                                  By adopting a voxel-based representation together with a\\nspecialized merging strategy, our method reduces the token count\\nby 193× compared with the original mesh format.\\n\\ning a new tokenizer for sim-ready physical 3D generation.\\n2.2. Articulated and Physical 3D Object Generation\\nArticulated object generation has attracted increasing at-\\ntention due to its wide range of applications.                                     Most exist-\\ning methods are retrieval-based: they first define a source\\nlibrary and then retrieve meshes from it to construct ar-\\nticulated 3D assets [11, 16].                                                           Other works adopt graph-\\nstructured representations [18, 20], combining the kine-\\nmatic graph of an articulated object with diffusion mod-\\nels to enable shape generation without texture.                                                         However,\\nthese approaches struggle to', 'metadata': {'file_path': 'D:\\\\Narwal\\\\mcp_rag\\\\data\\\\paper1.pdf', 'file_name': 'paper1.pdf', 'file_type': 'application/pdf', 'file_size': 4994615, 'creation_date': '2025-12-22', 'last_modified_date': '2025-12-22'}, 'chunk_index': 0, 'source_doc': '59308638-c26d-4533-9909-fff3e971ef2a'}, vector=None, shard_key=None, order_value=None),\n",
       " ScoredPoint(id='98df5d1b-c752-4e14-ae2f-5c4ac127159f', version=40, score=0.794876, payload={'text': '\\nReference    Ours    VGGT                                          Pi3    Fast3R\\n\\nFigure 6 Comparisons of point cloud quality. Our model produces point clouds that are more geometrically regular\\nand substantially less noisy than those generated by other methods.\\n\\n       RGB  Ours                   VGGT                      Pi3    Fast3R     MapAnything\\n\\nFigure 7 Comparisons of depth quality. Compared with other methods, our depth maps exhibit finer structural\\ndetail and higher semantic correctness across diverse scenes.\\n\\nsurpass complex task-specific designs. The advantage stems from large-scale pretraining, which enables better\\ngeneralization and scalability than approaches relying on epipolar transformers, cost volumes, or cascaded\\nmodules. Within this group, NVS performance correlates with geometry estimation capability, making\\nDA3 the strongest backbone. Looking forward, we expect FF-NVS can be effectively addressed with simple\\narchitectures leveraging pretrained geometry backbones, and that the strong spatial understanding of DA3\\nwill benefit other 3D vision tasks.\\n\\n7.2    Analysis for Depth Anything 3\\nTraining our DA3-Giant model requires 128×H100 GPUs for approximately 10 days. To reduce carbon\\nfootprint and computational cost, all ablation experiments reported in this section are conducted using the\\nViT-L backbone with a maximum of 10 views, requiring approximately 4 days on 32×H100 GPUs.\\n\\n7.2.1  Sufficiency of the Depth-Ray Representation\\nTo validate our depth-ray representation, we compare different prediction combinations summarized in Tab. 6.\\nAll models use a ViT-L backbone, identical training settings (view size: 10, batch size: 128, steps: 120k). We\\nevaluate four heads: 1) depth for dense depth maps; 2) pcd for direct 3D point clouds; 3) cam for 9-DoF\\ncamera pose c = (t, q, f); and 4) our proposed ray, predicting per-pixel ray maps (Sec. 3.1). The ray head\\nuses a Dual-DPT architecture, while pcd uses a separate DPT head. For models without pcd, point clouds\\n\\n                                         16\\n', 'metadata': {'file_path': 'D:\\\\Narwal\\\\mcp_rag\\\\data\\\\paper2.pdf', 'file_name': 'paper2.pdf', 'file_type': 'application/pdf', 'file_size': 20403091, 'creation_date': '2025-12-22', 'last_modified_date': '2025-12-22'}, 'chunk_index': 0, 'source_doc': '083660e6-7c3d-4fd4-91d4-253c32254fdb'}, vector=None, shard_key=None, order_value=None)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_encoder_retrival"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbaabea",
   "metadata": {},
   "source": [
    " Cross-Encoder reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ce4a3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, requests\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "VOYAGE_API_KEY = os.environ[\"VOYAGE_API_KEY\"]\n",
    "url = \"https://api.voyageai.com/v1/rerank\"\n",
    "\n",
    "def rerank(query: str, chunks: list[str], top_k: int = 5):\n",
    "    payload = {\n",
    "        \"model\": \"rerank-2.5\",   # or \"rerank-2.5\"\n",
    "        \"query\": query,\n",
    "        \"documents\": chunks,\n",
    "        \"top_k\": top_k,\n",
    "    }\n",
    "    headers = {\"Authorization\": f\"Bearer {VOYAGE_API_KEY}\"}\n",
    "    r = requests.post(url, json=payload, headers=headers)\n",
    "    r.raise_for_status()\n",
    "    return r.json()\n",
    "query = \"What is Model Context Protocol?\"\n",
    "\n",
    "chunks = [\n",
    "    \"Model Context Protocol (MCP) is a standard that allows large language models to call tools using a structured interface.\",\n",
    "    \"FastAPI is a modern Python web framework used to build APIs quickly and efficiently.\",\n",
    "    \"MCP enables LLMs to interact with external systems through JSON-RPC style messages.\",\n",
    "    \"Qdrant is a vector database designed for similarity search and retrieval.\",\n",
    "    \"The protocol defines how models expose tools and how clients invoke them.\"\n",
    "]\n",
    "result = rerank(query, chunks, top_k=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd6dec12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-\\nsidering both geometry and all physical attributes.                                In to-\\ntal, we collect 1,568 valid scores from 14 volunteers and\\nnormalize the scores. The results show that the outputs of\\nPhysX-Anything align much better with human preferences\\nthan those of other methods, confirming its robust gener-\\native performance in both geometry and physical proper-\\n\\n                                                                                                     6\\n',\n",
       " '\\n    Input    PhysX-Anything (Voxel)    PhysX-Anything (Index)    PhysX-Anything (Ours)\\n             Geometry    Kinematic     Geometry    Kinematic     Geometry    Kinematic\\n\\n              Question: Find Dimension: 90*60*100                              Dimension: 90*60*100                                        Dimension: 90*60*100\\n              a wheel of the shopping cart made of rubber                      Question: Find a wheel of the shopping cart made of rubber  Question: Find a wheel of the shopping cart made of rubber\\n                             1                      1                   7.8    1                 1                         7.8             1                    1               7.8\\n                             0.5                    0.5                 3.4    0.5               0.5                       3.4             0.5                  0.5             3.4\\n\\n              Affordance     0      Description     0      Material     1.1    Affordance  0     0                         1.1             0                    0               1.1\\n                                                                                                Description    Material                    Affordance    Description    Material\\n\\n    Figure 7. Ablation studies on different representation. We compare the generative performance of different 3D representations, which\\n    validates both the effectiveness and efficiency of our proposed representation.\\n              Faucet Switch Manipulation                                       Door Opening and Closing                                    Eyeglass Temple Folding\\n    #2        #32                   #63                    #23                 #53                    #67                  #13                           #29            #71\\n\\n    Lighter Snapping Open    Laptop Closing    Handle Manipulation\\n    #8    #31    #77         #8    #43         #70    #15    #42    #86\\n\\n    Figure 8.  Robot Manipulation on Generated sim-reaady 3D assets of PhysX-Anything. The results show that our generated sim-ready\\n    assets exhibit highly physically plausible behavior and accurate geometric structure across diverse tasks, thereby providing a new direction\\n    for robotics policy learning.\\n    Table 5.   Comparison with different representations. Quantitative results across different 3D representations clearly demonstrate the\\n    superiority of our proposed representation in both geometric fidelity and physical attributes.\\n\\n    Methods                  PSNR ↑ Geometry                          Physical Attributes\\n                                       CD ↓    F-Score ↑    Absolute scale ↓    Material ↑    Affordance ↑    Kinematic parameters (VLM) ↑    Description ↑\\n    PhysX-Anything-Voxel     16.96     17.81     63.10            0.40            12.32          11.63                    0.39                    17.38\\n    PhysX-Anything-Index     18.21     16.27     68.70            0.30            13.35          12.04                    0.76                    17.97\\n    PhysX-Anything (Ours)    20.35     14.43     77.50            0.30            17.52          14.28                    0.94                    19.36\\n\\nnets, lighters, eyeglasses, and other everyday objects—can\\nbe directly imported into the simulator and used for contact-\\nrich robotics policy learning.     This experiment not only\\ndemonstrates the physically plausible behavior and accu-\\nrate geometry of our generated assets, but also highlights\\ntheir strong potential to enable and inspire a wide range of\\ndownstream robotics and embodied AI applications.\\n\\n5. Conclusion\\n\\nIn this paper, we aim to fully unlock the potential of syn-\\nthesized 3D assets in real-world applications by introduc-\\ning PhysX-Anything, the first sim-ready physical 3D gen-\\nerative paradigm. Through a unified VLM-based pipeline\\nand a tailored 3D representation, PhysX-Anything achieves\\nsubstantial token compression (over 193×) while preserv-\\ning explicit geometric structure, enabling efficient and scal-\\nable physical 3D generation. In addition, to enrich the diver-\\nsity of existing physical 3D datasets, we construct PhysX-\\nMobility by carefully collecting and annotating common\\nreal-world objects with rich physical attributes.             It in-\\ncludes 47 the most common real-life categories with de-\\ntailed physical attributes.             Comprehensive experiments on\\nPhysX-Mobility and in-the-wild scenarios demonstrate the\\nstrong performance and robust generalization of PhysX-\\nAnything in sim-ready physical 3D generation.        Further-\\nmore, simulation-based experiments highlight its potential\\nfor downstream robotic policy learning. We believe PhysX-\\nAnything will spur new research directions across 3D vi-\\nsion, embodied AI and robotics.\\n\\n                                                                     ',\n",
       " '\\n    Input Image    URDFormer    Articulate-Anything    PhysXGen    PhysX-Anything (Ours)\\n                   Geometry    Kinematic    Geometry    Kinematic    Geometry    Kinematic    Geometry    Kinematic\\n\\n                                                         PhysXGen                                                                                         PhysX-Anything (Ours)\\n                      Dimension: 79.6*56.85*4.54         Question: Find the main ceramic structure of the toilet.            Dimension: 35*25*2           Question: Find the main ceramic structure of the toilet.\\n     1                                                                               1                               2.2                           1                           1                                  2.4\\n\\n                                                  0.5                   0.5                                          1.2                           0.5                         0.5                                1.2\\n\\n     0                                                                               0                               0.2                           0                           0                                  0\\n                       Affordance                        Description           Material                                      Affordance                   Description                 Material\\n                      Dimension: 58.6*47.3*9.97          Question: Find the metal outlet at the end of the tube              Dimension: 30*25*5           Question: Find the metal outlet at the end of the tube\\n     1                                                                               1                               7.6                           1                           1                                  7.8\\n\\n                                                  0.5                   0.5                                          5.25                          0.5                         0.5                                5.3\\n\\n     0                                                                               0                               2.9                           0                           0                                  2.9\\n                       Affordance                        Description           Material                                      Affordance                   Description                 Material\\n    Figure 6.   Qualitative results on in-the-wild images. Given a single real-world image as input, PhysX-Anything produces high-quality\\n    sim-ready 3D assets with realistic geometry, articulation, and physical attributes across diverse object categories. Moreover, the results\\n    highlight the robust generalization of PhysX-Anything.\\n\\n    Table 3.    User studies on in-the-wild evaluation.                 User preference results on in-the-wild cases show that PhysX-Anything significantly\\n    outperforms other methods, achieving a clear margin of improvement in geometry quality and physical plausibility.\\n\\n     Methods              Geometry (Human) ↑                            Absolute scale ↑    Material ↑                       Physical Attributes (Human)\\n                                                                                                                     Affordance ↑                  Kinematic parameters ↑      Description ↑\\n     URDFormer [11]              0.21                                          –                –                         –                                 0.23                     –\\n     Articulate-Anything [16]    0.53                                          –                –                         –                                 0.37                     –\\n     PhysXGen [3]                0.61                                         0.48             0.43                      0.34                               0.32                    0.33\\n     PhysX-Anything (Ours)       0.98                                         0.95             0.84                      0.94                               0.98                    0.96\\n\\n    Table 4.    In-the-wild VLM-based evaluation.                       Quantitative re-                           Fig. 3. Note that the original mesh and vertex-quantization\\n    sults from GPT-5 also confirm the strong generative performance                                                representations require an excessively large number of to-\\n    of PhysX-Anything in terms of geometry and articulation.                                                       kens, making end-to-end training infeasible due to out-of-\\n\\nMethods               Geometry (VLM) ↑                              Kinematic parameters (VLM) ↑\\n\\nURDFormer [11]              0.29                                                0.31\\nArticulate-Anything [16]    0.61                                                0.64\\nPhysXGen [3]                0.65                                                0.61\\nPhysX-Anything (Ours)       0.94                                                0.94\\n\\nties.  The visualizations in Figure 6 on real-life scenarios\\nfurther highlight the superiority of PhysX-Anythingagainst\\nother methods, showing more accurate geometry, articula-\\ntion, and physical attributes across diverse and challenging\\nin-the-wild cases.\\nmemory issues.          Therefore, we focus our comparison on\\nthe remaining three compact representations. As shown in\\nTable 5, as the token compression ratio increases, PhysX-\\nAnything is able to capture complete and detailed geometry\\n\\neven for complex structures, whereas alternative represen-\\ntations are constrained by the token budget and suffer no-\\nticeable degradation. The qualitative results in Fig. 7 fur-\\nther show that our PhysX-Anything produces more robust\\nresults for geometrically challenging objects.\\n\\n4.4. Robotic Policy Learning in Simulation\\n\\n    4.3.',\n",
       " ' for\\n• downstream applications in simulation and embodied AI.\\n We propose a unified VLM-based generative pipeline to-\\n gether with a    novel physical 3D representation. Our\\n representation compresses geometry tokens at a high rate\\n while preserving explicit geometric structure, and avoids\\n• introducing any special tokens during fine-tuning.\\n We construct a new physically grounded 3D dataset,\\n PhysX-Mobility, which enriches the category diversity\\n of existing physical 3D datasets by over  2×, including\\n over 2K common real-world objects such as cameras, cof-\\n• fee machines, and staplers.\\n Through comprehensive evaluations on PhysX-Mobility\\n and in-the-wild images, we demonstrate the strong gen-\\n erative quality  and robust generalization of      PhysX-\\n Anything. Furthermore, we validate the feasibility of di-\\n rectly deploying our sim-ready assets in simulation envi-\\n ronments, thereby empowering downstream applications\\n such as embodied AI and robotic manipulation.\\n2. Related Works\\n\\n2.1. 3D Generative Models\\n\\nAs one of the earliest paradigms for 3D generation, gen-\\nerative adversarial networks (GANs) played a central role\\nin the early stage of this field [6, 13].  However, they\\n\\n                                                                         2\\n',\n",
       " ' robustly generalize to novel\\nstructures, unseen categories, and complex texture. Drea-\\nmArt [21] instead attempts to optimize articulated 3D ob-\\njects from video generation outputs, but it requires manually\\nannotated part masks and becomes unstable when handling\\nobjects with many movable parts. URDF-Anything [19] can\\ndirectly generate URDF files. However, it relies on robust\\npoint cloud inputs and is hard to generate detailed texture\\nfor 3D assets.            Although some works attempt to learn the\\nphysical deformation of 3D assets [7, 8, 15, 17], they either\\ntreat all objects as homogeneous or ignore some key physi-\\ncal attributes. To push 3D generation toward physical real-\\nism, PhysXGen [3] first proposes a unified framework that\\ndirectly generates 3D assets with essential physical proper-\\nties, including absolute dimension, density, and so on. De-\\nspite its promising performance in physical 3D generation,\\nthere remains a substantial gap between the synthesized as-\\nsets and the requirements of modern physics simulators, re-\\nsulting in limited direct usability in downstream tasks.\\nTo fully realize the downstream utility of synthetic 3D\\nassets, we introduce the first 3D generation paradigm that,\\nfrom a single real-world image, produces high-quality sim-\\nready 3D assets equipped with explicit physical properties.\\nWe compare PhysX-Anything with existing approaches in\\nTable 1, which highlights that our method is the only one\\nthat simultaneously supports articulation, physical model-\\n\\ning, strong generalization, and simulation-ready deploy-\\n\\n\\nment. We believe that our approach offers a new direction\\n\\n\\nfor using synthetic data to empower related applications.\\n\\n3. Methodology\\n\\nIn this section, we present the detailed paradigm of PhysX-\\n\\nAnything, as illustrated in Fig. 3. It adopts a global-to-local\\n\\npipeline. Specifically, given a real-world image, PhysX-\\n\\nAnything conducts a multi-round conversation to sequen-\\ntially generate the overall physical description and the geo-\\nmetric information of each part. To mitigate context forget-\\nting caused by overly long prompts, we retain only the over-\\nall information when generating per-part geometry. In other\\nwords, the geometric descriptions of different parts are gen-\\nerated independently, conditioned solely on the shared over-\\nall information. Finally, by decoding the physical represen-\\ntation, PhysX-Anything can output simulation-ready physi-\\ncal 3D assets in six commonly used formats.\\n3.1. Physical Representation\\nPreviously, to reduce the token length of raw 3D meshes\\nin VLM-based frameworks, most 3D generation meth-\\nods [12, 26] adopt text-serialized representations based on\\nvertex quantization. However, the resulting token sequences\\nremain excessively long. Although 3D VQ-GAN [31] can\\nfurther compress geometric tokens, it requires introducing\\nadditional special tokens and a customized tokenizer during\\nfine-tuning, which complicates training and deployment.\\nTo address these limitations, we propose a new 3D rep-\\nresentation that substantially reduces token length while\\npreserving explicit geometric structure, without introducing\\nany additional tokenizer. Motivated by the impressive trade-\\noff between fidelity and efficiency of voxel-based represen-\\ntations [28], we build our representation on voxels. Directly\\nencoding high-resolution voxels, however, still yields an\\nunaffordable number of tokens for VLMs, even after map-\\nping geometry to a compressed space. We therefore adopt\\na coarse-to-fine strategy for geometry modeling: the VLM\\noperates on a 323 voxel grid to capture coarse geometry,\\nwhile a downstream decoder refines this coarse shape into\\nhigh-fidelity geometry. In this way, we retain the explicit\\nstructural advantages of 3D voxels while avoiding exces-\\nsive token consumption. As shown in Fig. 3, converting\\nmeshes to coarse voxels alone reduces the number of to-\\nkens by 74×.  To further eliminate redundancy in sparse\\nvoxel data, we linearize the 323 grid into indices from 0\\nto 323 − 1 and serialize only occupied voxels. Finally, by\\nmerging neighboring occupied indices and connecting con-\\ntinuous ranges with a hyphen −, we achieve an even higher\\ntoken compression rate (193×) while maintaining explicit\\n\\n                                                                                                                             4\\n',\n",
       " '\\n    range of downstream applications, especially in embodied              corresponding URDF & XML structure, yielding sim-ready\\n    AI and physics-based simulation.                                      assets that can be directly imported into standard simulators.\\n                                                                           Additionally, to significantly enrich the diversity of ex-\\n\\n1. Introduction\\n\\nFor a broad range of downstream applications in robotics,\\nembodied AI, and interactive simulation, there is an increas-\\ning demand for high-quality physical 3D assets that can be\\ndirectly executed in simulators. However, most existing 3D\\ngeneration methods either focus on global 3D geometry and\\nvisual appearance [10, 12, 14, 26, 28, 31], or on part-aware\\ngeneration [30, 33] that models object hierarchies and fine-\\ngrained structures. Despite their visually impressive perfor-\\nmance, the resulting assets typically lack essential physi-\\ncal and articulation information—such as density, absolute\\nscale, and joint constraints—which creates a substantial gap\\nto real-world applications and makes these assets difficult to\\ndeploy directly in simulators or physics engines.\\nIn parallel, a few works have started to explore the gener-\\nation of articulated objects [11, 16, 20, 21]. Yet, due to the\\nscarcity of large-scale high-quality annotated 3D datasets,\\nmany of these methods adopt retrieval-based paradigms:\\nthey retrieve an existing 3D model and attach plausible\\nmotions, rather than synthesizing fully novel, physically\\ngrounded assets. As a result, they provide only limited ar-\\nticulation information, generalize poorly to in-the-wild im-\\nages, and still lack the physical attributes required for realis-\\ntic simulation. While prior efforts attempt to learn deforma-\\ntion behavior for 3D assets [7, 8, 15, 17], they often impose\\na homogeneous-material assumption or neglect some es-\\nsential physical attributes. Even PhysXGen [3], which can\\ndirectly generate physical 3D assets, does not yet support\\nplug-and-play deployment in standard simulators or physics\\nengines [25, 27], thereby constraining its practical utility for\\ndownstream embodied AI and control tasks.\\nTo bridge  the     gap between      synthetic  3D    assets\\nand real downstream applications,   we propose       PhysX-\\nAnything—the first simulation-ready (sim-ready) phys-\\nical 3D generative paradigm. Given a single in-the-wild\\nimage, PhysX-Anything produces a high-quality sim-ready\\n3D asset, as illustrated in Fig. 1. Specifically, we introduce\\nthe first unified VLM-based generative model that jointly\\npredicts geometry, articulation structure, and essential phys-\\nical properties. Meanwhile, to resolve the intrinsic tension\\nbetween the limited token budget of VLMs and the com-\\nplexity of detailed 3D geometry, we design a new 3D rep-\\nresentation that tokenizes geometry efficiently. This repre-\\nsentation reduces the number of tokens by 193×, making it\\nfeasible to learn explicit geometry directly while avoiding\\nthe introduction of special tokens and new tokenizer during\\nfine-tuning. Based on the coarse geometry generated by the\\nVLM, we further develop a controllable flow transformer\\nand decoder to synthesize fine-grained geometry and the\\nisting physically grounded 3D datasets [3], we build a\\nnew dataset, PhysX-Mobility, by collecting assets from\\nPartNet-Mobility [27] and carefully annotating their phys-\\nical attributes.  PhysX-Mobility spans 47 categories and\\ncovers common real-world objects such as toilets, fans,\\ncameras, coffee machines, and staplers, thereby substan-\\ntially broadening the category coverage of physical 3D as-\\nsets. Comprehensive experiments on PhysX-Mobility, in-\\nthe-wild images, and user studies demonstrate that PhysX-\\nAnything achieves strong generative quality and robust gen-\\neralization compared with recent state-of-the-art methods.\\nFurthermore, to validate executability in standard simu-\\nlators and physics engines, we conduct experiments in a\\nMuJoCo-style simulator, showing that our sim-ready assets\\ncan be directly used in robotic policy learning for contact-\\nrich tasks, such as safe manipulation of delicate objects like\\neyeglasses. We believe our work opens up new possibilities\\nand directions for future research in 3D generation, embod-\\nied AI, and robotics.\\n• To summarize, our main contributions are:\\n We introduce PhysX-Anything, the first sim-ready phys-\\n ical 3D generative paradigm that,  given a single in-\\n the-wild image, produces high-quality sim-ready 3D as-\\n sets, thereby pushing the frontier of physically grounded\\n 3D content creation and unlocking new possibilities',\n",
       " ' introduced the SDS loss, which lever-\\nages the strong prior of 2D diffusion models to achieve\\nimpressive text-driven 3D generation quality.\\nless, optimization-based methods still suffer from the multi-\\nface Janus problem and low optimization efficiency.\\ncently, feed-forward methods have become the mainstream\\nin 3D generation due to their favorable efficiency and ro-\\nbustness [2, 4, 5, 10, 14, 24, 28, 29].\\ngenerative              per-          based models, several works introduce autoregressive mod-\\n             Subsequently,            eling into 3D generation [9, 23]. Motivated by the strong\\n                                      performance of vision–language models (VLMs), recent\\n                                      approaches have begun to employ VLMs to generate 3D as-\\n                        Neverthe-     sets. To limit the token length, LLaMA-Mesh [26] adopts a\\n                                      simplified mesh representation, upon which MeshLLM [12]\\n                              Re-     builds a part-to-assembly pipeline to further improve gen-\\n                                      erative quality.     Instead of using a simplified mesh repre-\\n                                      sentation, ShapeLLM-Omni [31] adopts a 3D VQ-VAE to\\nBeyond diffusion-                     compress the token sequence length, but at the cost of in-\\n\\n    Table 1.                                            Comparison of representative methods and their capa-     troducing additional special tokens and a new tokenizer for\\n    bilities. Gen. represents the generalization of methods. It shows                                            geometry, which complicates the training procedure.\\n    that our PhysX-Anything is the only approach that simultaneously                                                   In contrast to prior work, to better unlock the potential of\\n    satisfies all four criteria.                                                                                 VLMs for 3D generation, we propose a new, efficient repre-\\n\\nMethods                              Paradigm Articulate Physical Gen. Sim-ready\\nURDFormer .etc [11, 16, 20]  Retrieval    ✓    ✗    ✗    ✗\\nTrellis .etc [10, 14, 28]    Diffusion    ✗    ✗    ✓    ✗\\nMeshLLM .etc [12, 26, 31]       VLM       ✗    ✗    ✓    ✗\\nPhysXGen [3]                 Diffusion    ✓    ✓    ✓    ✗\\nPhysX-Anything                  VLM       ✓    ✓    ✓    ✓\\nsentation that substantially compresses the token sequence\\nwhile preserving explicit structural information. Moreover,\\nour approach introduces no additional special tokens during\\nfine-tuning, thereby avoiding both the need for large-scale\\n\\ntask-specific pretraining datasets and the overhead of train-\\n\\n    3\\n',\n",
       " 'ation, fine-grained voxel target, Gaussian noise, im-                 Beyond the quantitative comparison, we further present\\n    age condition, time step, and the controllable flow trans-              qualitative results in Fig. 5.     It clearly highlight the supe-\\n    former parameterized by θ, respectively. The noisy sample               riority of PhysX-Anything in terms of generalization, es-\\n    xt is obtained by interpolating between x0 and ϵ, i.e.,xt         =     pecially when compared with retrieval-based methods [11,\\n    (1 − t)x0 + tϵ .                                                        16].      Leveraging the powerful VLM prior and efficient\\n    Given the fine-grained voxel representation, we adopt a                 representation, PhysX-Anything also produces significantly\\n    pre-trained structured latent diffusion model [28] to gen-              more plausible physical attributes than PhysXGen [3].\\n\\n                                                                           5\\n',\n",
       " '\\n                 Avg Token of Parts=177450                           Avg Token of Part=118490\\nv    ⎵      0 .       0 2  ⎵ 0 .  7 1     ⎵     0  .    0  5         v     ⎵    8  ⎵  3     0       ⎵    1    2\\n     ⎵                     ⎵              ⎵                     Quantization\\nv           0 .       0 3    0 .  7 2           0  .    0  2         v     ⎵    1  3  ⎵     3       1    ⎵    1     2\\nMesh ⎵                     ⎵              ⎵                          v     ⎵ ⎵        ⎵ ⎵                ⎵ ⎵\\nv           0 .       0 3    0 .  7 2           0  .    0  3                    1  3        3       1         1     2\\n     ⎵                     ⎵              ⎵                                ⎵          ⎵                  ⎵\\nOriginal                                                             v\\nv           0 .       0 3    0 .  7 2           0  .    0  4                    1  3        3       1         1     2\\n     ⎵                     ⎵              ⎵                          v     ⎵          ⎵                  ⎵\\nv           0 .       0 8    0 .  7 4           0  .    0  2 Vertex             1  3        3       1         1     2\\n…                                                                    …\\nf    ⎵      1 ⎵       2 ⎵  3        ⎵ represents Spaces              f     ⎵    1  ⎵  2          ⎵  3\\n…                                                                    …\\n            Avg Token of Part=2377  Avg Token of Parts=1628                            Avg Token of Part=919\\n8    ⎵      3 0       ⎵ 1  2 ⎵           9     1  6    4   ⎵                     9 1   6    4       ⎵\\n            ⎵           ⎵      ⎵  Voxel index                   ⎵\\nVoxelization  3       1    1 2           1     4  3    1   6                     1 4   3    1       6    -\\n1    3      ⎵    ⎵        ⎵                       ⎵             ⎵         Ours        ⎵                  ⎵\\n1    3      ⎵ 3       1 ⎵  1 3 ⎵         1     4  3    1   7    ⎵                1 4   3    1       8\\n1    3      ⎵ 3       1 ⎵  1 4 ⎵         1     4  3    1   8    ⎵                2…3   3    5       5    ⎵\\n2    2      ⎵ 2       5 ⎵  2 7 ⎵         2     3  3    5   5    ⎵\\n…                                        …                                ~193 × Tokens Reduction\\nFigure 3.        Comparison of token counts between representa-\\ntions.                                                  By adopting a voxel-based representation together with a\\nspecialized merging strategy, our method reduces the token count\\nby 193× compared with the original mesh format.\\n\\ning a new tokenizer for sim-ready physical 3D generation.\\n2.2. Articulated and Physical 3D Object Generation\\nArticulated object generation has attracted increasing at-\\ntention due to its wide range of applications.                                     Most exist-\\ning methods are retrieval-based: they first define a source\\nlibrary and then retrieve meshes from it to construct ar-\\nticulated 3D assets [11, 16].                                                           Other works adopt graph-\\nstructured representations [18, 20], combining the kine-\\nmatic graph of an articulated object with diffusion mod-\\nels to enable shape generation without texture.                                                         However,\\nthese approaches struggle to',\n",
       " '\\nReference    Ours    VGGT                                          Pi3    Fast3R\\n\\nFigure 6 Comparisons of point cloud quality. Our model produces point clouds that are more geometrically regular\\nand substantially less noisy than those generated by other methods.\\n\\n       RGB  Ours                   VGGT                      Pi3    Fast3R     MapAnything\\n\\nFigure 7 Comparisons of depth quality. Compared with other methods, our depth maps exhibit finer structural\\ndetail and higher semantic correctness across diverse scenes.\\n\\nsurpass complex task-specific designs. The advantage stems from large-scale pretraining, which enables better\\ngeneralization and scalability than approaches relying on epipolar transformers, cost volumes, or cascaded\\nmodules. Within this group, NVS performance correlates with geometry estimation capability, making\\nDA3 the strongest backbone. Looking forward, we expect FF-NVS can be effectively addressed with simple\\narchitectures leveraging pretrained geometry backbones, and that the strong spatial understanding of DA3\\nwill benefit other 3D vision tasks.\\n\\n7.2    Analysis for Depth Anything 3\\nTraining our DA3-Giant model requires 128×H100 GPUs for approximately 10 days. To reduce carbon\\nfootprint and computational cost, all ablation experiments reported in this section are conducted using the\\nViT-L backbone with a maximum of 10 views, requiring approximately 4 days on 32×H100 GPUs.\\n\\n7.2.1  Sufficiency of the Depth-Ray Representation\\nTo validate our depth-ray representation, we compare different prediction combinations summarized in Tab. 6.\\nAll models use a ViT-L backbone, identical training settings (view size: 10, batch size: 128, steps: 120k). We\\nevaluate four heads: 1) depth for dense depth maps; 2) pcd for direct 3D point clouds; 3) cam for 9-DoF\\ncamera pose c = (t, q, f); and 4) our proposed ray, predicting per-pixel ray maps (Sec. 3.1). The ray head\\nuses a Dual-DPT architecture, while pcd uses a separate DPT head. For models without pcd, point clouds\\n\\n                                         16\\n']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_encoder_chunks =[i.payload[\"text\"] for i in bi_encoder_retrival]\n",
    "cross_encoder_metadata = [i.payload[\"metadata\"] for i in bi_encoder_retrival]\n",
    "cross_encoder_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "37b53e0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'object': 'list',\n",
       " 'data': [{'relevance_score': 0.8359375, 'index': 8},\n",
       "  {'relevance_score': 0.8203125, 'index': 4},\n",
       "  {'relevance_score': 0.7890625, 'index': 5}],\n",
       " 'model': 'rerank-2.5',\n",
       " 'usage': {'total_tokens': 6893}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question  = \"By what factor does the proposed 3D representation in PhysX-Anything reduce the number of geometry tokens?\"\n",
    "cross_encoder_rerank = rerank(question, cross_encoder_chunks, top_k=3)\n",
    "cross_encoder_rerank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa8276e",
   "metadata": {},
   "source": [
    "Late Interaction reranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "99b93c5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\n                 Avg Token of Parts=177450                           Avg Token of Part=118490\\nv    ⎵      0 .       0 2  ⎵ 0 .  7 1     ⎵     0  .    0  5         v     ⎵    8  ⎵  3     0       ⎵    1    2\\n     ⎵                     ⎵              ⎵                     Quantization\\nv           0 .       0 3    0 .  7 2           0  .    0  2         v     ⎵    1  3  ⎵     3       1    ⎵    1     2\\nMesh ⎵                     ⎵              ⎵                          v     ⎵ ⎵        ⎵ ⎵                ⎵ ⎵\\nv           0 .       0 3    0 .  7 2           0  .    0  3                    1  3        3       1         1     2\\n     ⎵                     ⎵              ⎵                                ⎵          ⎵                  ⎵\\nOriginal                                                             v\\nv           0 .       0 3    0 .  7 2           0  .    0  4                    1  3        3       1         1     2\\n     ⎵                     ⎵              ⎵                          v     ⎵          ⎵                  ⎵\\nv           0 .       0 8    0 .  7 4           0  .    0  2 Vertex             1  3        3       1         1     2\\n…                                                                    …\\nf    ⎵      1 ⎵       2 ⎵  3        ⎵ represents Spaces              f     ⎵    1  ⎵  2          ⎵  3\\n…                                                                    …\\n            Avg Token of Part=2377  Avg Token of Parts=1628                            Avg Token of Part=919\\n8    ⎵      3 0       ⎵ 1  2 ⎵           9     1  6    4   ⎵                     9 1   6    4       ⎵\\n            ⎵           ⎵      ⎵  Voxel index                   ⎵\\nVoxelization  3       1    1 2           1     4  3    1   6                     1 4   3    1       6    -\\n1    3      ⎵    ⎵        ⎵                       ⎵             ⎵         Ours        ⎵                  ⎵\\n1    3      ⎵ 3       1 ⎵  1 3 ⎵         1     4  3    1   7    ⎵                1 4   3    1       8\\n1    3      ⎵ 3       1 ⎵  1 4 ⎵         1     4  3    1   8    ⎵                2…3   3    5       5    ⎵\\n2    2      ⎵ 2       5 ⎵  2 7 ⎵         2     3  3    5   5    ⎵\\n…                                        …                                ~193 × Tokens Reduction\\nFigure 3.        Comparison of token counts between representa-\\ntions.                                                  By adopting a voxel-based representation together with a\\nspecialized merging strategy, our method reduces the token count\\nby 193× compared with the original mesh format.\\n\\ning a new tokenizer for sim-ready physical 3D generation.\\n2.2. Articulated and Physical 3D Object Generation\\nArticulated object generation has attracted increasing at-\\ntention due to its wide range of applications.                                     Most exist-\\ning methods are retrieval-based: they first define a source\\nlibrary and then retrieve meshes from it to construct ar-\\nticulated 3D assets [11, 16].                                                           Other works adopt graph-\\nstructured representations [18, 20], combining the kine-\\nmatic graph of an articulated object with diffusion mod-\\nels to enable shape generation without texture.                                                         However,\\nthese approaches struggle to',\n",
       " ' robustly generalize to novel\\nstructures, unseen categories, and complex texture. Drea-\\nmArt [21] instead attempts to optimize articulated 3D ob-\\njects from video generation outputs, but it requires manually\\nannotated part masks and becomes unstable when handling\\nobjects with many movable parts. URDF-Anything [19] can\\ndirectly generate URDF files. However, it relies on robust\\npoint cloud inputs and is hard to generate detailed texture\\nfor 3D assets.            Although some works attempt to learn the\\nphysical deformation of 3D assets [7, 8, 15, 17], they either\\ntreat all objects as homogeneous or ignore some key physi-\\ncal attributes. To push 3D generation toward physical real-\\nism, PhysXGen [3] first proposes a unified framework that\\ndirectly generates 3D assets with essential physical proper-\\nties, including absolute dimension, density, and so on. De-\\nspite its promising performance in physical 3D generation,\\nthere remains a substantial gap between the synthesized as-\\nsets and the requirements of modern physics simulators, re-\\nsulting in limited direct usability in downstream tasks.\\nTo fully realize the downstream utility of synthetic 3D\\nassets, we introduce the first 3D generation paradigm that,\\nfrom a single real-world image, produces high-quality sim-\\nready 3D assets equipped with explicit physical properties.\\nWe compare PhysX-Anything with existing approaches in\\nTable 1, which highlights that our method is the only one\\nthat simultaneously supports articulation, physical model-\\n\\ning, strong generalization, and simulation-ready deploy-\\n\\n\\nment. We believe that our approach offers a new direction\\n\\n\\nfor using synthetic data to empower related applications.\\n\\n3. Methodology\\n\\nIn this section, we present the detailed paradigm of PhysX-\\n\\nAnything, as illustrated in Fig. 3. It adopts a global-to-local\\n\\npipeline. Specifically, given a real-world image, PhysX-\\n\\nAnything conducts a multi-round conversation to sequen-\\ntially generate the overall physical description and the geo-\\nmetric information of each part. To mitigate context forget-\\nting caused by overly long prompts, we retain only the over-\\nall information when generating per-part geometry. In other\\nwords, the geometric descriptions of different parts are gen-\\nerated independently, conditioned solely on the shared over-\\nall information. Finally, by decoding the physical represen-\\ntation, PhysX-Anything can output simulation-ready physi-\\ncal 3D assets in six commonly used formats.\\n3.1. Physical Representation\\nPreviously, to reduce the token length of raw 3D meshes\\nin VLM-based frameworks, most 3D generation meth-\\nods [12, 26] adopt text-serialized representations based on\\nvertex quantization. However, the resulting token sequences\\nremain excessively long. Although 3D VQ-GAN [31] can\\nfurther compress geometric tokens, it requires introducing\\nadditional special tokens and a customized tokenizer during\\nfine-tuning, which complicates training and deployment.\\nTo address these limitations, we propose a new 3D rep-\\nresentation that substantially reduces token length while\\npreserving explicit geometric structure, without introducing\\nany additional tokenizer. Motivated by the impressive trade-\\noff between fidelity and efficiency of voxel-based represen-\\ntations [28], we build our representation on voxels. Directly\\nencoding high-resolution voxels, however, still yields an\\nunaffordable number of tokens for VLMs, even after map-\\nping geometry to a compressed space. We therefore adopt\\na coarse-to-fine strategy for geometry modeling: the VLM\\noperates on a 323 voxel grid to capture coarse geometry,\\nwhile a downstream decoder refines this coarse shape into\\nhigh-fidelity geometry. In this way, we retain the explicit\\nstructural advantages of 3D voxels while avoiding exces-\\nsive token consumption. As shown in Fig. 3, converting\\nmeshes to coarse voxels alone reduces the number of to-\\nkens by 74×.  To further eliminate redundancy in sparse\\nvoxel data, we linearize the 323 grid into indices from 0\\nto 323 − 1 and serialize only occupied voxels. Finally, by\\nmerging neighboring occupied indices and connecting con-\\ntinuous ranges with a hyphen −, we achieve an even higher\\ntoken compression rate (193×) while maintaining explicit\\n\\n                                                                                                                             4\\n',\n",
       " '\\n    range of downstream applications, especially in embodied              corresponding URDF & XML structure, yielding sim-ready\\n    AI and physics-based simulation.                                      assets that can be directly imported into standard simulators.\\n                                                                           Additionally, to significantly enrich the diversity of ex-\\n\\n1. Introduction\\n\\nFor a broad range of downstream applications in robotics,\\nembodied AI, and interactive simulation, there is an increas-\\ning demand for high-quality physical 3D assets that can be\\ndirectly executed in simulators. However, most existing 3D\\ngeneration methods either focus on global 3D geometry and\\nvisual appearance [10, 12, 14, 26, 28, 31], or on part-aware\\ngeneration [30, 33] that models object hierarchies and fine-\\ngrained structures. Despite their visually impressive perfor-\\nmance, the resulting assets typically lack essential physi-\\ncal and articulation information—such as density, absolute\\nscale, and joint constraints—which creates a substantial gap\\nto real-world applications and makes these assets difficult to\\ndeploy directly in simulators or physics engines.\\nIn parallel, a few works have started to explore the gener-\\nation of articulated objects [11, 16, 20, 21]. Yet, due to the\\nscarcity of large-scale high-quality annotated 3D datasets,\\nmany of these methods adopt retrieval-based paradigms:\\nthey retrieve an existing 3D model and attach plausible\\nmotions, rather than synthesizing fully novel, physically\\ngrounded assets. As a result, they provide only limited ar-\\nticulation information, generalize poorly to in-the-wild im-\\nages, and still lack the physical attributes required for realis-\\ntic simulation. While prior efforts attempt to learn deforma-\\ntion behavior for 3D assets [7, 8, 15, 17], they often impose\\na homogeneous-material assumption or neglect some es-\\nsential physical attributes. Even PhysXGen [3], which can\\ndirectly generate physical 3D assets, does not yet support\\nplug-and-play deployment in standard simulators or physics\\nengines [25, 27], thereby constraining its practical utility for\\ndownstream embodied AI and control tasks.\\nTo bridge  the     gap between      synthetic  3D    assets\\nand real downstream applications,   we propose       PhysX-\\nAnything—the first simulation-ready (sim-ready) phys-\\nical 3D generative paradigm. Given a single in-the-wild\\nimage, PhysX-Anything produces a high-quality sim-ready\\n3D asset, as illustrated in Fig. 1. Specifically, we introduce\\nthe first unified VLM-based generative model that jointly\\npredicts geometry, articulation structure, and essential phys-\\nical properties. Meanwhile, to resolve the intrinsic tension\\nbetween the limited token budget of VLMs and the com-\\nplexity of detailed 3D geometry, we design a new 3D rep-\\nresentation that tokenizes geometry efficiently. This repre-\\nsentation reduces the number of tokens by 193×, making it\\nfeasible to learn explicit geometry directly while avoiding\\nthe introduction of special tokens and new tokenizer during\\nfine-tuning. Based on the coarse geometry generated by the\\nVLM, we further develop a controllable flow transformer\\nand decoder to synthesize fine-grained geometry and the\\nisting physically grounded 3D datasets [3], we build a\\nnew dataset, PhysX-Mobility, by collecting assets from\\nPartNet-Mobility [27] and carefully annotating their phys-\\nical attributes.  PhysX-Mobility spans 47 categories and\\ncovers common real-world objects such as toilets, fans,\\ncameras, coffee machines, and staplers, thereby substan-\\ntially broadening the category coverage of physical 3D as-\\nsets. Comprehensive experiments on PhysX-Mobility, in-\\nthe-wild images, and user studies demonstrate that PhysX-\\nAnything achieves strong generative quality and robust gen-\\neralization compared with recent state-of-the-art methods.\\nFurthermore, to validate executability in standard simu-\\nlators and physics engines, we conduct experiments in a\\nMuJoCo-style simulator, showing that our sim-ready assets\\ncan be directly used in robotic policy learning for contact-\\nrich tasks, such as safe manipulation of delicate objects like\\neyeglasses. We believe our work opens up new possibilities\\nand directions for future research in 3D generation, embod-\\nied AI, and robotics.\\n• To summarize, our main contributions are:\\n We introduce PhysX-Anything, the first sim-ready phys-\\n ical 3D generative paradigm that,  given a single in-\\n the-wild image, produces high-quality sim-ready 3D as-\\n sets, thereby pushing the frontier of physically grounded\\n 3D content creation and unlocking new possibilities']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_encoder_rerank_chunks = []\n",
    "for i in cross_encoder_rerank[\"data\"]:\n",
    "    index = i.get(\"index\")\n",
    "    cross_encoder_rerank_chunks.append(cross_encoder_chunks[index])\n",
    "\n",
    "cross_encoder_rerank_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e525a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Narwal\\mcp_rag\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Narwal\\mcp_rag\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\sriha\\.cache\\huggingface\\hub\\models--colbert-ir--colbertv2.0. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Load ColBERTv2 model & tokenizer\n",
    "colbert_tokenizer = AutoTokenizer.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "colbert_model = AutoModel.from_pretrained(\"colbert-ir/colbertv2.0\")\n",
    "colbert_model.eval()\n",
    "\n",
    "def get_token_embeddings(text, tokenizer, model, max_length=128):\n",
    "    # Get token-level embeddings, ignore [CLS] and [SEP]\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=max_length)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    # outputs.last_hidden_state: (1, seq_len, hidden_dim)\n",
    "    input_ids = inputs['input_ids'][0]\n",
    "    keep_indices = (input_ids != tokenizer.cls_token_id) & (input_ids != tokenizer.sep_token_id)\n",
    "\n",
    "    return outputs.last_hidden_state[0][keep_indices]  # (filtered_seq_len, hidden_dim)\n",
    "\n",
    "def colbert_score(query_emb, doc_emb):\n",
    "    # query_emb: (m, d), doc_emb: (n, d)\n",
    "    sim = F.cosine_similarity(query_emb.unsqueeze(1), doc_emb.unsqueeze(0), dim=2)  # (m, n)\n",
    "    max_sim, _ = sim.max(dim=1)  # (m,)\n",
    "    return max_sim.sum().item()\n",
    "\n",
    "def rerank_with_late_interaction(query, chunks, top_k=5):\n",
    "    # Precompute query embeddings once\n",
    "    query_emb = get_token_embeddings(query, colbert_tokenizer, colbert_model)\n",
    "\n",
    "    scores = []\n",
    "    for i, ch in enumerate(chunks):\n",
    "        doc_emb = get_token_embeddings(ch, colbert_tokenizer, colbert_model)\n",
    "        score = colbert_score(query_emb, doc_emb)\n",
    "        scores.append((score, i))  # keep index so you can map back to metadata\n",
    "\n",
    "    scores.sort(reverse=True)  # highest score first\n",
    "    top = scores[:top_k]\n",
    "\n",
    "    # return list of dicts with index + score (index maps to original chunks list)\n",
    "    return [{\"index\": i, \"late_interaction_score\": float(s)} for (s, i) in top]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c82c8481",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'index': 3, 'late_interaction_score': 16.465396881103516},\n",
       " {'index': 0, 'late_interaction_score': 14.276972770690918},\n",
       " {'index': 2, 'late_interaction_score': 13.568941116333008}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "late_interaction_rank  = rerank_with_late_interaction(question, cross_encoder_chunks, top_k=3)\n",
    "late_interaction_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "117b8717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' for\\n• downstream applications in simulation and embodied AI.\\n We propose a unified VLM-based generative pipeline to-\\n gether with a    novel physical 3D representation. Our\\n representation compresses geometry tokens at a high rate\\n while preserving explicit geometric structure, and avoids\\n• introducing any special tokens during fine-tuning.\\n We construct a new physically grounded 3D dataset,\\n PhysX-Mobility, which enriches the category diversity\\n of existing physical 3D datasets by over  2×, including\\n over 2K common real-world objects such as cameras, cof-\\n• fee machines, and staplers.\\n Through comprehensive evaluations on PhysX-Mobility\\n and in-the-wild images, we demonstrate the strong gen-\\n erative quality  and robust generalization of      PhysX-\\n Anything. Furthermore, we validate the feasibility of di-\\n rectly deploying our sim-ready assets in simulation envi-\\n ronments, thereby empowering downstream applications\\n such as embodied AI and robotic manipulation.\\n2. Related Works\\n\\n2.1. 3D Generative Models\\n\\nAs one of the earliest paradigms for 3D generation, gen-\\nerative adversarial networks (GANs) played a central role\\nin the early stage of this field [6, 13].  However, they\\n\\n                                                                         2\\n',\n",
       " '-\\nsidering both geometry and all physical attributes.                                In to-\\ntal, we collect 1,568 valid scores from 14 volunteers and\\nnormalize the scores. The results show that the outputs of\\nPhysX-Anything align much better with human preferences\\nthan those of other methods, confirming its robust gener-\\native performance in both geometry and physical proper-\\n\\n                                                                                                     6\\n',\n",
       " '\\n    Input Image    URDFormer    Articulate-Anything    PhysXGen    PhysX-Anything (Ours)\\n                   Geometry    Kinematic    Geometry    Kinematic    Geometry    Kinematic    Geometry    Kinematic\\n\\n                                                         PhysXGen                                                                                         PhysX-Anything (Ours)\\n                      Dimension: 79.6*56.85*4.54         Question: Find the main ceramic structure of the toilet.            Dimension: 35*25*2           Question: Find the main ceramic structure of the toilet.\\n     1                                                                               1                               2.2                           1                           1                                  2.4\\n\\n                                                  0.5                   0.5                                          1.2                           0.5                         0.5                                1.2\\n\\n     0                                                                               0                               0.2                           0                           0                                  0\\n                       Affordance                        Description           Material                                      Affordance                   Description                 Material\\n                      Dimension: 58.6*47.3*9.97          Question: Find the metal outlet at the end of the tube              Dimension: 30*25*5           Question: Find the metal outlet at the end of the tube\\n     1                                                                               1                               7.6                           1                           1                                  7.8\\n\\n                                                  0.5                   0.5                                          5.25                          0.5                         0.5                                5.3\\n\\n     0                                                                               0                               2.9                           0                           0                                  2.9\\n                       Affordance                        Description           Material                                      Affordance                   Description                 Material\\n    Figure 6.   Qualitative results on in-the-wild images. Given a single real-world image as input, PhysX-Anything produces high-quality\\n    sim-ready 3D assets with realistic geometry, articulation, and physical attributes across diverse object categories. Moreover, the results\\n    highlight the robust generalization of PhysX-Anything.\\n\\n    Table 3.    User studies on in-the-wild evaluation.                 User preference results on in-the-wild cases show that PhysX-Anything significantly\\n    outperforms other methods, achieving a clear margin of improvement in geometry quality and physical plausibility.\\n\\n     Methods              Geometry (Human) ↑                            Absolute scale ↑    Material ↑                       Physical Attributes (Human)\\n                                                                                                                     Affordance ↑                  Kinematic parameters ↑      Description ↑\\n     URDFormer [11]              0.21                                          –                –                         –                                 0.23                     –\\n     Articulate-Anything [16]    0.53                                          –                –                         –                                 0.37                     –\\n     PhysXGen [3]                0.61                                         0.48             0.43                      0.34                               0.32                    0.33\\n     PhysX-Anything (Ours)       0.98                                         0.95             0.84                      0.94                               0.98                    0.96\\n\\n    Table 4.    In-the-wild VLM-based evaluation.                       Quantitative re-                           Fig. 3. Note that the original mesh and vertex-quantization\\n    sults from GPT-5 also confirm the strong generative performance                                                representations require an excessively large number of to-\\n    of PhysX-Anything in terms of geometry and articulation.                                                       kens, making end-to-end training infeasible due to out-of-\\n\\nMethods               Geometry (VLM) ↑                              Kinematic parameters (VLM) ↑\\n\\nURDFormer [11]              0.29                                                0.31\\nArticulate-Anything [16]    0.61                                                0.64\\nPhysXGen [3]                0.65                                                0.61\\nPhysX-Anything (Ours)       0.94                                                0.94\\n\\nties.  The visualizations in Figure 6 on real-life scenarios\\nfurther highlight the superiority of PhysX-Anythingagainst\\nother methods, showing more accurate geometry, articula-\\ntion, and physical attributes across diverse and challenging\\nin-the-wild cases.\\nmemory issues.          Therefore, we focus our comparison on\\nthe remaining three compact representations. As shown in\\nTable 5, as the token compression ratio increases, PhysX-\\nAnything is able to capture complete and detailed geometry\\n\\neven for complex structures, whereas alternative represen-\\ntations are constrained by the token budget and suffer no-\\nticeable degradation. The qualitative results in Fig. 7 fur-\\nther show that our PhysX-Anything produces more robust\\nresults for geometrically challenging objects.\\n\\n4.4. Robotic Policy Learning in Simulation\\n\\n    4.3.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "late_interaction_rerank_chunks = []\n",
    "for i in late_interaction_rank:\n",
    "    index = i.get(\"index\")\n",
    "    late_interaction_rerank_chunks.append(cross_encoder_chunks[index])\n",
    "late_interaction_rerank_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c3685773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def gpt_rag_answer(query: str, chunks: list[str], model: str = \"gpt-4o-mini\") -> str:\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "      - query: user question (string)\n",
    "      - chunks: list of retrieved/reranked text chunks (list[str])\n",
    "\n",
    "    Output:\n",
    "      - final answer string from GPT-4o-mini\n",
    "    \"\"\"\n",
    "    context = \"\\n\\n---\\n\\n\".join(chunks)\n",
    "\n",
    "    prompt = f\"\"\"Use ONLY the provided chunks to answer the question.\n",
    "\n",
    "CHUNKS:\n",
    "{context}\n",
    "\n",
    "QUESTION: {query}\n",
    "\n",
    "Rules:\n",
    "- If the answer is not in the chunks, say: \"Not found in provided context.\"\n",
    "- Be specific and grounded in the chunks.\n",
    "\"\"\"\n",
    "\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "        temperature=0.2,\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9b4d0d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[12/26/25 15:36:00] </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> HTTP Request: <span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">POST</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff; text-decoration: underline\">https://api.openai.com/v1/chat/completions</span>          <a href=\"file://d:\\Narwal\\mcp_rag\\.venv\\Lib\\site-packages\\httpx\\_client.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_client.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file://d:\\Narwal\\mcp_rag\\.venv\\Lib\\site-packages\\httpx\\_client.py#1025\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1025</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">\"HTTP/1.1 200 OK\"</span>                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">               </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[12/26/25 15:36:00]\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m HTTP Request: \u001b[1;33mPOST\u001b[0m \u001b[4;94mhttps://api.openai.com/v1/chat/completions\u001b[0m          \u001b]8;id=654182;file://d:\\Narwal\\mcp_rag\\.venv\\Lib\\site-packages\\httpx\\_client.py\u001b\\\u001b[2m_client.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=250035;file://d:\\Narwal\\mcp_rag\\.venv\\Lib\\site-packages\\httpx\\_client.py#1025\u001b\\\u001b[2m1025\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m\"HTTP/1.1 200 OK\"\u001b[0m                                                      \u001b[2m               \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'The proposed 3D representation in PhysX-Anything reduces the number of geometry tokens by a factor of 193×.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_rag_answer(question, cross_encoder_rerank_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52dc61c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcp_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
